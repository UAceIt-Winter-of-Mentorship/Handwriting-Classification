{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eaAn-okaWA1",
        "outputId": "1182fd1d-3e3a-4638-8f0d-da023b24dd23"
      },
      "id": "3eaAn-okaWA1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c94734",
      "metadata": {
        "id": "21c94734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9138f29-ab06-463c-ad57-703c7cc52483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5491662080769983669\n",
            "xla_global_id: -1\n",
            "]\n",
            "Device mapping: no known devices.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\t\t\t\t#opencv library\n",
        "import os\t\t\t\t#to load files present in os\n",
        "import pandas as pd\t\t\t\n",
        "import string\n",
        "import matplotlib.pyplot as plt\t\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#importing keras for neural networks\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\t\t#activations functions\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\t\t\t\t#utility functions\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#from keras_tqdm import TQDMNotebookCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\t\t#to split dataset into train and test\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#import tensorflow for CNN, also checking the version available\n",
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from random import *\n",
        "from PIL import Image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU, Activation, BatchNormalization\n",
        "from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "from subprocess import check_output"
      ],
      "metadata": {
        "id": "v3UGzwEQekqp"
      },
      "id": "v3UGzwEQekqp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = './data_subset/data_subset/*'\n",
        "path_to_files = os.path.join('./data_subset/data_subset/', '*')     #add path for dataset files\n",
        "# d = {}\n",
        "# with open('forms_for_parsing.txt') as f:\n",
        "#     for line in f:\n",
        "#         key = line.split(' ')[0]\n",
        "#         writer = line.split(' ')[1]\n",
        "#         d[key] = writer\n",
        "# print(len(d.keys()))"
      ],
      "metadata": {
        "id": "xChNxlH0etOa"
      },
      "id": "xChNxlH0etOa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_files = []\n",
        "target_list = []\n",
        "\n",
        "# Uses glob module to find file pathnames\n",
        "for filepath in sorted(glob.glob(path_to_files)):\n",
        "    temp_files.append(filepath)\n",
        "    image_name = filepath.split('\\\\')[-1]\n",
        "    file, ext = os.path.splitext(image_name)\n",
        "    parts = file.split('-')\n",
        "    form = parts[0] + '-' + parts[1]\n",
        "    \n",
        "    target_list.append(d[form])\n",
        "\n",
        "img_files = np.asarray(temp_files)\n",
        "img_targets = np.asarray(target_list)\n",
        "\n",
        "img_files.shape, img_targets.shape"
      ],
      "metadata": {
        "id": "h27A0we7e0kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d913c501-1f03-4b36-c610-b578a02eb3b4"
      },
      "id": "h27A0we7e0kN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0,), (0,))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in img_files[:3]:\n",
        "    img=mpimg.imread(filename)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(img, cmap ='gray')\n",
        "\n",
        "np.save('img_files.npy', img_files)\n",
        "np.save('img_targets.npy', img_targets)"
      ],
      "metadata": {
        "id": "thx31WPOe3au"
      },
      "id": "thx31WPOe3au",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoded_targets = encoder.fit_transform(img_targets)\n",
        "\n",
        "# print(img_files[0])\n",
        "# print(img_targets[0])\n",
        "# print(encoded_targets[0])"
      ],
      "metadata": {
        "id": "c0vFuTDJe9P4"
      },
      "id": "c0vFuTDJe9P4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files, rem_files, train_targets, rem_targets = train_test_split(img_files, encoded_targets, train_size=0.66, random_state=88, shuffle= True)\n",
        "\n",
        "validation_files, test_files, validation_targets, test_targets = train_test_split(\n",
        "        rem_files, rem_targets, train_size=0.5, random_state=8, shuffle=True)\n",
        "\n",
        "print(train_files.shape, validation_files.shape, test_files.shape)\n",
        "print(train_targets.shape, validation_targets.shape, test_targets.shape)"
      ],
      "metadata": {
        "id": "Y1dfQcnofB3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "14d7f577-aed4-4d3f-e70a-e85301d506be"
      },
      "id": "Y1dfQcnofB3q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6b36b43d0f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.66\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m validation_files, test_files, validation_targets, test_targets = train_test_split(\n\u001b[1;32m      4\u001b[0m         rem_files, rem_targets, train_size=0.5, random_state=8, shuffle=True)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m     n_train, n_test = _validate_shuffle_split(\n\u001b[0;32m-> 2421\u001b[0;31m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m     )\n\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m             \u001b[0;34m\"aforementioned parameters.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2102\u001b[0m         )\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=None and train_size=0.66, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8 #16\n",
        "num_classes = 50\n",
        "\n",
        "# Start with train generator shared in the class and add image augmentations\n",
        "def generate_data(samples, target_files,  batch_size=batch_size, factor = 0.1 ):\n",
        "    num_samples = len(samples)\n",
        "    from sklearn.utils import shuffle\n",
        "    while 1: # Loop forever so the generator never terminates\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_samples = samples[offset:offset+batch_size]\n",
        "            batch_targets = target_files[offset:offset+batch_size]\n",
        "\n",
        "            images = []\n",
        "            targets = []\n",
        "            for i in range(len(batch_samples)):\n",
        "                batch_sample = batch_samples[i]\n",
        "                batch_target = batch_targets[i]\n",
        "                im = Image.open(batch_sample)\n",
        "                cur_width = im.size[0]\n",
        "                cur_height = im.size[1]\n",
        "\n",
        "                # print(cur_width, cur_height)\n",
        "                height_fac = 113 / cur_height\n",
        "\n",
        "                new_width = int(cur_width * height_fac)\n",
        "                size = new_width, 113\n",
        "\n",
        "                imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n",
        "                now_width = imresize.size[0]\n",
        "                now_height = imresize.size[1]\n",
        "                # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n",
        "\n",
        "                avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n",
        "\n",
        "                # Pick random x%\n",
        "                pick_num = int(len(avail_x_points)*factor)\n",
        "\n",
        "                # Now pick\n",
        "                random_startx = sample(avail_x_points,  pick_num)\n",
        "\n",
        "                for start in random_startx:\n",
        "                    imcrop = imresize.crop((start, 0, start+113, 113))\n",
        "                    images.append(np.asarray(imcrop))\n",
        "                    targets.append(batch_target)\n",
        "\n",
        "            # trim image to only see section with road\n",
        "            X_train = np.array(images)\n",
        "            y_train = np.array(targets)\n",
        "\n",
        "            #reshape X_train for feeding in later\n",
        "            X_train = X_train.reshape(X_train.shape[0], 113, 113, 1)\n",
        "            #convert to float and normalize\n",
        "            X_train = X_train.astype('float32')\n",
        "            X_train /= 255\n",
        "\n",
        "            #One hot encode y\n",
        "            y_train = to_categorical(y_train, num_classes)\n",
        "            yield shuffle(X_train, y_train)"
      ],
      "metadata": {
        "id": "P7Obx5kofGIC"
      },
      "id": "P7Obx5kofGIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61348b2",
      "metadata": {
        "id": "b61348b2",
        "outputId": "17440fa8-14df-4def-c22e-18e86b2ade32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-16429b5d9efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#we have loaded a file which contains all the words present, this helps in indentification and matching of words in images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C:\\Users\\LAALASA\\OneDrive\\Jupyter\\english-words-master\\english-words-master\\words.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LAALASA\\\\OneDrive\\\\Jupyter\\\\english-words-master\\\\english-words-master\\\\words.txt'"
          ]
        }
      ],
      "source": [
        "#we have loaded a file which contains all the words present, this helps in indentification and matching of words in images\n",
        "with open(r'C:\\Users\\LAALASA\\OneDrive\\Jupyter\\english-words-master\\english-words-master\\words.txt') as f:\n",
        "    contents = f.readlines()\n",
        "lines = [line.strip() for line in contents] \n",
        "lines[0]\n",
        "\n",
        "\n",
        "max_label_len = 0\n",
        "\n",
        "char_list = \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" \n",
        "\n",
        "# string.ascii_letters + string.digits (Chars & Digits)\n",
        "# or \n",
        "# \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "print(char_list, len(char_list))\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, chara in enumerate(txt):\n",
        "        dig_lst.append(char_list.index(chara))\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "#declaring some variables\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "RECORDS_COUNT = 10000\n",
        "train_images = []\n",
        "train_labels = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "train_original_text = []\n",
        "\n",
        "valid_images = []\n",
        "valid_labels = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_original_text = []\n",
        "\n",
        "inputs_length = []\n",
        "labels_length = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ee8c59",
      "metadata": {
        "id": "c3ee8c59"
      },
      "outputs": [],
      "source": [
        "def process_image(img):\n",
        "    \"\"\"\n",
        "\tPreprocessing image is a necessary step, it is done before we give them as input to CNN\n",
        "\tWe make all the images of similar sizes and normalize them i.e. to convert them into numpy arrays\n",
        "    Converts image to shape (32, 128, 1) & normalize\n",
        "    \"\"\"\n",
        "    w, h = img.shape\n",
        "    \n",
        "#     _, img = cv2.threshold(img, \n",
        "#                            128, \n",
        "#                            255, \n",
        "#                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    \n",
        "    # Aspect Ratio Calculation\n",
        "    new_w = 32\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h = img.shape\n",
        "    \n",
        "    img = img.astype('float32')\n",
        "    \n",
        "    # Converts each to (32, 128, 1)\n",
        "    if w < 32:\n",
        "        add_zeros = np.full((32-w, h), 255)\n",
        "        img = np.concatenate((img, add_zeros))\n",
        "        w, h = img.shape\n",
        "    \n",
        "    if h < 128:\n",
        "        add_zeros = np.full((w, 128-h), 255)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "        w, h = img.shape\n",
        "        \n",
        "    if h > 128 or w > 32:\n",
        "        dim = (128,32)\n",
        "        img = cv2.resize(img, dim)\n",
        "    \n",
        "    img = cv2.subtract(255, img)\n",
        "    \n",
        "    img = np.expand_dims(img, axis=2)\n",
        "    \n",
        "    # Normalize \n",
        "    img = img / 255\n",
        "    \n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c031a1e",
      "metadata": {
        "id": "0c031a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "456de025-35b6-4f57-e685-742566a0561a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-da7608906e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train and validation set generation and split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
          ]
        }
      ],
      "source": [
        "#Train and validation set generation and split\n",
        "for index, line in enumerate(lines):\n",
        "    splits = line.split(' ')\n",
        "    status = splits[0]\n",
        "    \n",
        "    if status == 'ok':\n",
        "        word_id = splits[0]\n",
        "        word = \"\".join(splits[8:])\n",
        "        \n",
        "        splits_id = word_id.split('-')\n",
        "        filepath = 'words/{}/{}-{}/{}.png'.format(splits_id[0], \n",
        "                                                  splits_id[0], \n",
        "                                                  splits_id[1], \n",
        "                                                  word_id)\n",
        "        \n",
        "        # process image\n",
        "        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        try:\n",
        "            img = process_image(img)\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "        # process label\n",
        "        try:\n",
        "            label = encode_to_labels(word)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        if index % 10 == 0:\n",
        "            valid_images.append(img)\n",
        "            valid_labels.append(label)\n",
        "            valid_input_length.append(31)\n",
        "            valid_label_length.append(len(word))\n",
        "            valid_original_text.append(word)\n",
        "        else:\n",
        "            train_images.append(img)\n",
        "            train_labels.append(label)\n",
        "            train_input_length.append(31)\n",
        "            train_label_length.append(len(word))\n",
        "            train_original_text.append(word)\n",
        "        \n",
        "        if len(word) > max_label_len:\n",
        "            max_label_len = len(word)\n",
        "    \n",
        "    if index >= RECORDS_COUNT:\n",
        "        break\n",
        "\n",
        "train_padded_label = pad_sequences(train_labels, \n",
        "                             maxlen=max_label_len, \n",
        "                             padding='post',\n",
        "                             value=len(char_list))\n",
        "\n",
        "valid_padded_label = pad_sequences(valid_labels, \n",
        "                             maxlen=max_label_len, \n",
        "                             padding='post',\n",
        "                             value=len(char_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_label.shape, valid_padded_label.shape\n"
      ],
      "metadata": {
        "id": "w95WKrIkaCMd"
      },
      "id": "w95WKrIkaCMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc373d1",
      "metadata": {
        "id": "3bc373d1",
        "outputId": "5e8e22f2-b23f-4ca1-ded9-81dd3fa43412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 128, 1)]      0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 128, 64)       640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 64, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 64, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 32, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 32, 256)        295168    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 32, 256)        590080    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 32, 256)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 4, 32, 512)        1180160   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 4, 32, 512)       2048      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 32, 512)        2359808   \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4, 32, 512)       2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 2, 32, 512)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 1, 31, 512)        1049088   \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 31, 512)           0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 31, 512)          1574912   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 31, 512)          1574912   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 31, 79)            40527     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,743,247\n",
            "Trainable params: 8,741,199\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer \"ctc\" (type Lambda).\n\nInput tensor `ctc/Cast_2:0` enters the loop with shape (1, 0), but has shape (1, None) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\n\nCall arguments received:\n  • inputs=['tf.Tensor(shape=(None, 31, 79), dtype=float32)', 'tf.Tensor(shape=(None, 0), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=int64)', 'tf.Tensor(shape=(None, 1), dtype=int64)']\n  • mask=None\n  • training=False",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26260/885332440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctc_batch_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mloss_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctc_lambda_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ctc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#model to be used at training time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26260/885332440.py\u001b[0m in \u001b[0;36mctc_lambda_func\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctc_batch_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mloss_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctc_lambda_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ctc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"ctc\" (type Lambda).\n\nInput tensor `ctc/Cast_2:0` enters the loop with shape (1, 0), but has shape (1, None) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\n\nCall arguments received:\n  • inputs=['tf.Tensor(shape=(None, 31, 79), dtype=float32)', 'tf.Tensor(shape=(None, 0), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=int64)', 'tf.Tensor(shape=(None, 1), dtype=int64)']\n  • mask=None\n  • training=False"
          ]
        }
      ],
      "source": [
        "#print the shape\n",
        "train_padded_label.shape, valid_padded_label.shape\t\n",
        "\n",
        "#converting them to numpy\n",
        "train_images = np.asarray(train_images)\n",
        "train_input_length = np.asarray(train_input_length)\n",
        "train_label_length = np.asarray(train_label_length)\n",
        "\n",
        "valid_images = np.asarray(valid_images)\n",
        "valid_input_length = np.asarray(valid_input_length)\n",
        "valid_label_length = np.asarray(valid_label_length)\n",
        "\n",
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)\n",
        "act_model.summary()\n",
        "\n",
        "the_labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, the_labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, the_labels, input_length, label_length], outputs=loss_out)\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 30\n",
        "e = str(epochs)\n",
        "optimizer_name = 'sgd'\n",
        "\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = optimizer_name, metrics=['accuracy'])\n",
        "\n",
        "filepath=\"{}o-{}r-{}e-{}t-{}v.hdf5\".format(optimizer_name,\n",
        "                                          str(RECORDS_COUNT),\n",
        "                                          str(epochs),\n",
        "                                          str(train_images.shape[0]),\n",
        "                                          str(valid_images.shape[0]))\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "history = model.fit(x=[train_images, train_padded_label, train_input_length, train_label_length],\n",
        "                    y=np.zeros(len(train_images)),\n",
        "                    batch_size=batch_size, \n",
        "                    epochs=epochs, \n",
        "                    validation_data=([valid_images, valid_padded_label, valid_input_length, valid_label_length], [np.zeros(len(valid_images))]),\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks_list)\n",
        "\n",
        "# load the saved best model weights\n",
        "act_model.load_weights(filepath)\n",
        "\n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_images)\n",
        " \n",
        "# use CTC decoder\n",
        "decoded = K.ctc_decode(prediction, \n",
        "                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
        "                       greedy=True)[0][0]\n",
        "out = K.get_value(decoded)\n",
        "\n",
        "import Levenshtein as lv\n",
        "\n",
        "total_jaro = 0\n",
        "total_rati = 0\n",
        "# see the results\n",
        "for i, x in enumerate(out):\n",
        "    letters=''\n",
        "    for p in x:\n",
        "        if int(p) != -1:\n",
        "            letters+=char_list[int(p)]\n",
        "    total_jaro+=lv.jaro(letters, valid_original_text[i])\n",
        "    total_rati+=lv.ratio(letters, valid_original_text[i])\n",
        "\n",
        "print('jaro :', total_jaro/len(out))\n",
        "print('ratio:', total_rati/len(out))\n",
        "\n",
        "\n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(train_images[150:170])\n",
        " \n",
        "# use CTC decoder\n",
        "decoded = K.ctc_decode(prediction,   \n",
        "                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
        "                       greedy=True)[0][0]\n",
        "\n",
        "out = K.get_value(decoded)\n",
        "\n",
        "# see the results\n",
        "for i, x in enumerate(out):\n",
        "    print(\"original_text =  \", train_original_text[150+i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:\n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')\n",
        "    plt.imshow(train_images[150+i].reshape(32,128), cmap=plt.cm.gray)\n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "\n",
        "# plot accuracy and loss\n",
        "def plotgraph(epochs, acc, val_acc):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(epochs, acc, 'b')\n",
        "    plt.plot(epochs, val_acc, 'r')\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1,len(loss)+1)\n",
        "plotgraph(epochs, loss, val_loss)\n",
        "plotgraph(epochs, acc, val_acc)\n",
        "# get best model index\n",
        "minimum_val_loss = np.min(history.history['val_loss'])\n",
        "best_model_index = np.where(history.history['val_loss'] == minimum_val_loss)[0][0]\n",
        "\n",
        "best_loss = str(history.history['loss'][best_model_index])\n",
        "best_acc = str(history.history['accuracy'][best_model_index])\n",
        "best_val_loss = str(history.history['val_loss'][best_model_index])\n",
        "best_val_acc = str(history.history['val_accuracy'][best_model_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f5b878",
      "metadata": {
        "id": "96f5b878"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}